import streamlit as st
import numpy as np
import json
from groq import Groq
from openai import OpenAI
from openai import APIError
from sklearn.metrics.pairwise import cosine_similarity 

# --- KONFIGURACJA KLUCZY I INICJALIZACJA ---
# Wymaga kluczy: GROQ_API_KEY i OPENAI_API_KEY w Streamlit Secrets

st.set_page_config(layout="centered", page_title="Silnik Wiedzy RAG")

try:
    # Weryfikacja klucza GROQ dla LLM
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets. Jest wymagany dla LLM (Llama 3).")
    st.stop() 

try:
    # Weryfikacja klucza OPENAI dla embedding√≥w
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    if not OPENAI_API_KEY:
        st.error("B≈ÇƒÖd: Wymagana warto≈õƒá 'OPENAI_API_KEY' w Streamlit Secrets. U≈ºywamy go do wektoryzacji (Embedding√≥w).")
        st.stop()
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'OPENAI_API_KEY' w Streamlit Secrets. Jest WYMAGANY dla embedding√≥w.")
    st.stop()

try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# WBUDOWANA BAZA WIEDZY (Dla uproszczonego testu)
# ------------------------------
# Ta lista tekst√≥w s≈Çu≈ºy jako zastƒôpcza baza wiedzy RAG.
DOCUMENT_TEXTS = [
    "Python jest jƒôzykiem programowania u≈ºywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
    "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie.",
    "Groq oferuje bardzo szybkie modele AI dla programist√≥w, dzia≈ÇajƒÖce na akceleratorach LPU (Language Processing Unit).",
    "Podobie≈Ñstwo Kosinusowe (Cosine Similarity) mierzy kƒÖt miƒôdzy dwoma wektorami w przestrzeni, okre≈õlajƒÖc podobie≈Ñstwo semantyczne.",
    "RAG (Retrieval-Augmented Generation) to architektura AI, kt√≥ra wykorzystuje bazƒô wiedzy (retrieval) do ulepszania odpowiedzi LLM (generation).",
    "Do wspinaczki sportowej niezbƒôdna jest lina dynamiczna, uprzƒÖ≈º i ekspresy. Wa≈ºna jest technika wiƒÖzania √≥semki.",
    "Wspinaczka tradycyjna wymaga umiejƒôtno≈õci osadzania w≈Çasnej asekuracji, np. ko≈õci i friend√≥w. Jest to bardziej wymagajƒÖce psychicznie.",
]


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - WY≈ÅƒÑCZNIE OpenAI
# ------------------------------
@st.cache_data(show_spinner=False)
def compute_embeddings(texts):
    """Generuje embeddingi dla listy tekst√≥w u≈ºywajƒÖc modelu OpenAI (text-embedding-3-small)."""
    try:
        response = openai_client.embeddings.create(
            model="text-embedding-3-small", 
            input=texts
        )
        # Ekstrakcja wektor√≥w z odpowiedzi API
        embeddings = np.array([data.embedding for data in response.data])
        return embeddings

    except APIError as e:
        st.error(f"Krytyczny b≈ÇƒÖd API OpenAI (Embeddingi): {e}. Sprawd≈∫, czy klucz OPENAI_API_KEY jest poprawny.")
        # Podnoszenie wyjƒÖtku w celu zatrzymania aplikacji Streamlit
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Weryfikacja klucza OpenAI/kredyt√≥w.")
    except Exception as e:
        st.error(f"Nieoczekiwany b≈ÇƒÖd podczas generowania embedding√≥w OpenAI: {e}")
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Nieznany b≈ÇƒÖd.")


# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (Groq)."""
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM (Groq)."

# ------------------------------
# Simple semantic search (Cosine Similarity)
# ------------------------------
def search(query, doc_embeddings, doc_texts):
    """Wyszukuje najbardziej podobny dokument do zapytania za pomocƒÖ Podobie≈Ñstwa Kosinusowego."""
    try:
        # Konwersja zapytania na wektor
        query_emb_list = compute_embeddings([query])
    except RuntimeError:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0 
        
    if query_emb_list.size == 0:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # Obliczanie podobie≈Ñstwa kosinusowego
    # Wymagane jest rzutowanie na float64 dla poprawno≈õci
    similarities = cosine_similarity(query_emb.reshape(1, -1), doc_embeddings.astype(np.float64))
    best = np.argmax(similarities)
    
    # Zwracanie najlepiej dopasowanego tekstu i jego wyniku
    return doc_texts[best], similarities[0, best]

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("üß† Silnik Wiedzy ‚Äî Uproszczona Edycja RAG üöÄ")

st.write("LLM (Llama 3 70B) dzia≈Ça na Groq. Embeddingi dzia≈ÇajƒÖ na **stabilnym API OpenAI**.")
st.write("Ta wersja u≈ºywa **wbudowanej, ma≈Çej bazy wiedzy** w kodzie Python, nie pliku CSV.")
st.markdown("---")


# ≈Åadowanie i buforowanie embedding√≥w dokument√≥w
@st.cache_resource
def load_document_embeddings(texts):
    """Wczytuje embeddingi i zapewnia, ≈ºe aplikacja siƒô nie uruchomi, je≈õli to siƒô nie powiedzie."""
    st.subheader("Faza 1: Wczytywanie bazy wiedzy")
    with st.spinner(f"Generowanie embedding√≥w dla {len(texts)} dokument√≥w..."):
        try:
            emb = compute_embeddings(texts)
        except RuntimeError:
            st.warning("Aplikacja zosta≈Ça zatrzymana. Sprawd≈∫, czy klucze API sƒÖ poprawne.")
            st.stop()
            
        st.success("Baza wiedzy za≈Çadowana pomy≈õlnie!")
        return emb

DOCUMENT_EMB = load_document_embeddings(DOCUMENT_TEXTS)

# ------------------------------
# UI Input
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest RAG?):")

if query:
    if DOCUMENT_EMB.size == 0:
        st.warning("Nie mo≈ºna wykonaƒá wyszukiwania, poniewa≈º baza wiedzy jest pusta lub wystƒÖpi≈Ç b≈ÇƒÖd ≈Çadowania.")
    else:
        with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
            
            # Wyszukiwanie sematyczne
            best_doc, score = search(query, DOCUMENT_EMB, DOCUMENT_TEXTS)

            st.markdown("### üîé Znaleziony Kontekst (RAG Retrieval)")
            st.write(f"**Podobie≈Ñstwo (Cosine Score):** {score:.4f}")
            st.code(best_doc) 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym i wspinaczkowym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
            # Wywo≈Çanie LLM (Groq)
            answer = ask_llm(final_prompt)
            st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B - Groq)")
            st.info(answer)
