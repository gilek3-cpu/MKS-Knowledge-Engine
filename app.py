import streamlit as st
import numpy as np
import json
from groq import Groq

# --- KONFIGURACJA API ---
# Wymaga zmiennej Å›rodowiskowej GROQ_API_KEY w Streamlit Secrets
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    # WyÅ›wietla bÅ‚Ä…d, jeÅ›li klucz nie jest dostÄ™pny, i zatrzymuje aplikacjÄ™
    st.error("BÅ‚Ä…d konfiguracji: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets.")
    st.stop() 

# Inicjalizacja klienta Groq
try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"BÅ‚Ä…d inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - Groq
# ------------------------------
@st.cache_data
def compute_embeddings(texts):
    """
    Generuje embeddingi dla listy tekstÃ³w uÅ¼ywajÄ…c modelu Nomic Embed Text
    dostÄ™pnego przez Groq. UÅ¼ywa @st.cache_data, by cache'owaÄ‡ wyniki.
    """
    embeddings = []
    
    # Przetwarzanie tekstÃ³w w pÄ™tli
    for t in texts:
        try:
            # UÅ¼ywamy Nomic Embed Text, jedynego modelu embeddingÃ³w na Groq
            response = client.embeddings.create(
                model="nomic-embed-text",
                input=t
            )
            # Pobieramy wektor z obiektu odpowiedzi (zwraca listÄ™, bierzemy pierwszy element)
            embeddings.append(response.data[0].embedding)
        except Exception as e:
            st.error(f"Krytyczny bÅ‚Ä…d podczas generowania embeddingu dla tekstu: '{t[:30]}...'. BÅ‚Ä…d: {e}")
            return [] # ZwrÃ³Ä‡ pustÄ… listÄ™, aby aplikacja siÄ™ nie zawiesiÅ‚a
            
    return embeddings

# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """
    Generuje odpowiedÅº LLM na podstawie promptu, uÅ¼ywajÄ…c modelu Llama 3 70B (szybki).
    """
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", # Bardzo szybki model od Groq
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        # Poprawny dostÄ™p do odpowiedzi LLM: completion.choices[0].message.content
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas wywoÅ‚ania LLM Groq: {e}")
        return "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d w komunikacji z modelem LLM."

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("ğŸ§  Silnik Wiedzy â€” Groq Edition ğŸš€")

st.write("Embeddingi (Nomic Embed) + LLM (Llama 3 70B) dziaÅ‚ajÄ… teraz **w 100% na Groq API**.")
st.markdown("---")


# Example documents
# ZmieniÅ‚em na bardziej zÅ‚oÅ¼one dane, aby zademonstrowaÄ‡ dziaÅ‚anie RAG
DOCUMENT_TEXTS = [
    "Python jest jÄ™zykiem programowania uÅ¼ywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
    "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie bez znajomoÅ›ci HTML/CSS/JS.",
    "Groq oferuje bardzo szybkie i darmowe modele AI dla programistÃ³w, dziaÅ‚ajÄ…ce na specjalistycznych akceleratorach LPU (Language Processing Unit).",
    "Funkcja Cosine Similarity mierzy kÄ…t miÄ™dzy dwoma wektorami w przestrzeni, okreÅ›lajÄ…c podobieÅ„stwo semantyczne.",
]

# Åadowanie i buforowanie embeddingÃ³w
# Oddzielna funkcja do Å‚adowania, by mÃ³c zatrzymaÄ‡ aplikacjÄ™ w razie bÅ‚Ä™du
def load_document_embeddings():
    """Wczytuje embeddingi i zapewnia, Å¼e aplikacja siÄ™ nie uruchomi, jeÅ›li to siÄ™ nie powiedzie."""
    st.subheader("Faza 1: Wczytywanie bazy wiedzy")
    with st.spinner("Generowanie embeddingÃ³w dla dokumentÃ³w..."):
        emb = compute_embeddings(DOCUMENT_TEXTS)
        if not emb:
            st.error("Nie udaÅ‚o siÄ™ zaÅ‚adowaÄ‡ bazy wiedzy. SprawdÅº klucz Groq i logi bÅ‚Ä™dÃ³w.")
            st.stop()
        st.success("Baza wiedzy zaÅ‚adowana pomyÅ›lnie!")
        return emb

DOCUMENT_EMB = load_document_embeddings()

# ------------------------------
# Simple semantic search
# ------------------------------
def cosine_similarity(a, b):
    """Oblicza podobieÅ„stwo cosinusowe miÄ™dzy dwoma wektorami."""
    a = np.array(a)
    b = np.array(b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
        
    return np.dot(a, b) / (norm_a * norm_b)

def search(query):
    """Wyszukuje najbardziej podobny dokument do zapytania."""
    # 1. Generowanie embeddingu dla zapytania
    query_emb_list = compute_embeddings([query])
    if not query_emb_list:
        return "BÅ‚Ä…d generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # 2. Obliczanie podobieÅ„stwa
    sims = [cosine_similarity(query_emb, emb) for emb in DOCUMENT_EMB]
    best = np.argmax(sims)
    
    return DOCUMENT_TEXTS[best], sims[best]

# ------------------------------
# UI Input
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest Streamlit?):")

if query:
    if not DOCUMENT_EMB:
        # Ten warunek jest dodatkowym zabezpieczeniem, jeÅ›li st.stop() zawiedzie
        st.warning("Nie moÅ¼na wykonaÄ‡ wyszukiwania, poniewaÅ¼ baza wiedzy jest pusta.")
    else:
        with st.spinner("Szukam kontekstu i generujÄ™ odpowiedÅº..."):
            
            # Wyszukiwanie semantyczne
            best_doc, score = search(query)

            st.markdown("### ğŸ” Znaleziony Kontekst")
            st.write(f"**PodobieÅ„stwo (Cosine Score):** {score:.4f}")
            st.code(best_doc) 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            JesteÅ› ekspertem technicznym. UÅ¼yj **wyÅ‚Ä…cznie** poniÅ¼szego fragmentu wiedzy, 
            aby odpowiedzieÄ‡ na pytanie uÅ¼ytkownika. Odpowiadaj zwiÄ™Åºle i precyzyjnie. 
            JeÅ›li kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajÄ…cych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            OdpowiedÅº:
            """
    
            # WywoÅ‚anie LLM
            answer = ask_llm(final_prompt)
            st.markdown("### ğŸ¤– OdpowiedÅº Modelu (Llama 3 70B)")
            st.info(answer) 
