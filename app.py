import streamlit as st
import numpy as np
import json
from groq import Groq

# --- KONFIGURACJA API ---
# Wymaga zmiennej ≈õrodowiskowej GROQ_API_KEY w Streamlit Secrets
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    # Wy≈õwietla b≈ÇƒÖd, je≈õli klucz nie jest dostƒôpny, i zatrzymuje aplikacjƒô
    st.error("B≈ÇƒÖd konfiguracji: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets.")
    st.stop() 

# Inicjalizacja klienta Groq
try:
    # Upewnij siƒô, ≈ºe klucz jest u≈ºywany podczas inicjalizacji klienta
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - Groq
# ------------------------------
@st.cache_data
def compute_embeddings(texts):
    """
    Generuje embeddingi dla listy tekst√≥w u≈ºywajƒÖc modelu Nomic Embed Text
    dostƒôpnego przez Groq.
    """
    embeddings = []
    
    # Przetwarzanie tekst√≥w w pƒôtli
    for t in texts:
        try:
            # U≈ºywamy Nomic Embed Text
            response = client.embeddings.create(
                model="nomic-embed-text",
                input=t
            )
            # U≈ºywamy list() i enumerate() aby zapewniƒá, ≈ºe odpowied≈∫ jest poprawnie przetworzona
            for i, data in enumerate(response.data):
                embeddings.append(data.embedding)

        except Exception as e:
            # Zmieniamy komunikat, aby jeszcze raz zaznaczyƒá, ≈ºe to problem z kluczem/dostƒôpem
            st.error(f"Krytyczny b≈ÇƒÖd API Groq w compute_embeddings. Sprawd≈∫, czy klucz API jest POPRAWNY i AKTUALNY oraz czy model 'nomic-embed-text' jest dostƒôpny. Szczeg√≥≈Çy: {e}")
            # Rzucamy wyjƒÖtek, aby zako≈Ñczyƒá proces (jest to niezbƒôdne do poprawnego dzia≈Çania st.stop() w load_document_embeddings)
            raise RuntimeError("Nie uda≈Ço siƒô wygenerowaƒá embedding√≥w.")
            
    return embeddings

# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """
    Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (szybki).
    """
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", # Szybki model od Groq
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM."

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("üß† Silnik Wiedzy ‚Äî Groq Edition üöÄ")

st.write("Embeddingi (Nomic Embed) + LLM (Llama 3 70B) dzia≈ÇajƒÖ teraz **w 100% na Groq API**.")
st.markdown("---")


# Example documents
DOCUMENT_TEXTS = [
    "Python jest jƒôzykiem programowania u≈ºywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
    "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie bez znajomo≈õci HTML/CSS/JS.",
    "Groq oferuje bardzo szybkie i darmowe modele AI dla programist√≥w, dzia≈ÇajƒÖce na specjalistycznych akceleratorach LPU (Language Processing Unit).",
    "Funkcja Cosine Similarity mierzy kƒÖt miƒôdzy dwoma wektorami w przestrzeni, okre≈õlajƒÖc podobie≈Ñstwo semantyczne.",
]

# ≈Åadowanie i buforowanie embedding√≥w (z zabezpieczeniem)
@st.cache_resource
def load_document_embeddings():
    """Wczytuje embeddingi i zapewnia, ≈ºe aplikacja siƒô nie uruchomi, je≈õli to siƒô nie powiedzie."""
    st.subheader("Faza 1: Wczytywanie bazy wiedzy")
    with st.spinner("Generowanie embedding√≥w dla dokument√≥w..."):
        try:
            emb = compute_embeddings(DOCUMENT_TEXTS)
        except RuntimeError:
            # Wy≈õwietla b≈ÇƒÖd rzucony przez compute_embeddings
            st.error("Nie uda≈Ço siƒô za≈Çadowaƒá bazy wiedzy. Sprawd≈∫ klucz Groq i logi b≈Çƒôd√≥w.")
            st.stop()
            
        st.success("Baza wiedzy za≈Çadowana pomy≈õlnie!")
        return emb

DOCUMENT_EMB = load_document_embeddings()

# ------------------------------
# Simple semantic search (Cosine Similarity)
# ------------------------------
def cosine_similarity(a, b):
    """Oblicza podobie≈Ñstwo cosinusowe miƒôdzy dwoma wektorami."""
    a = np.array(a)
    b = np.array(b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
        
    return np.dot(a, b) / (norm_a * norm_b)

def search(query):
    """Wyszukuje najbardziej podobny dokument do zapytania."""
    # 1. Generowanie embeddingu dla zapytania
    try:
        query_emb_list = compute_embeddings([query])
    except RuntimeError:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # 2. Obliczanie podobie≈Ñstwa
    sims = [cosine_similarity(query_emb, emb) for emb in DOCUMENT_EMB]
    best = np.argmax(sims)
    
    return DOCUMENT_TEXTS[best], sims[best]

# ------------------------------
# UI Input
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest Streamlit?):")

if query:
    if not DOCUMENT_EMB:
        st.warning("Nie mo≈ºna wykonaƒá wyszukiwania, poniewa≈º baza wiedzy jest pusta.")
    else:
        with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
            
            # Wyszukiwanie semantyczne
            best_doc, score = search(query)

            st.markdown("### üîé Znaleziony Kontekst")
            st.write(f"**Podobie≈Ñstwo (Cosine Score):** {score:.4f}")
            st.code(best_doc) 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
            # Wywo≈Çanie LLM
            answer = ask_llm(final_prompt)
            st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B)")
            st.info(answer)
