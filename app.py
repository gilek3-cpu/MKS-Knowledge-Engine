import streamlit as st
import numpy as np
import json
import pandas as pd
from groq import Groq
from openai import OpenAI
from openai import APIError
from sklearn.metrics.pairwise import cosine_similarity 

# --- KONFIGURACJA KLUCZY I INICJALIZACJA ---

st.set_page_config(layout="centered", page_title="Silnik Wiedzy RAG")

# Sprawdzamy, czy klucze sƒÖ dostƒôpne w Streamlit Secrets
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    st.error("B≈ÇƒÖd konfiguracji: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets. Jest wymagany dla LLM (Llama 3).")
    st.stop() 

try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    if not OPENAI_API_KEY:
        st.error("B≈ÇƒÖd: Wymagana warto≈õƒá 'OPENAI_API_KEY' w Streamlit Secrets. U≈ºywamy go do wektoryzacji (Embedding√≥w).")
        st.stop()
    # Inicjalizacja klienta OpenAI (do embedding√≥w)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'OPENAI_API_KEY' w Streamlit Secrets. Jest WYMAGANY dla embedding√≥w.")
    st.stop()

# Inicjalizacja klienta Groq (dla LLM)
try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - WY≈ÅƒÑCZNIE OpenAI
# ------------------------------
@st.cache_data(show_spinner=False)
def compute_embeddings(texts):
    """
    Generuje embeddingi dla listy tekst√≥w u≈ºywajƒÖc modelu OpenAI
    (text-embedding-3-small).
    """
    try:
        # st.info("Using OpenAI (text-embedding-3-small) for embeddings...")
        response = openai_client.embeddings.create(
            model="text-embedding-3-small", 
            input=texts
        )
        # Pobieranie wektor√≥w
        embeddings = [data.embedding for data in response.data]
        return embeddings

    except APIError as e:
        st.error(f"Krytyczny b≈ÇƒÖd API OpenAI (Embeddingi): {e}. Sprawd≈∫, czy klucz OPENAI_API_KEY jest poprawny i czy masz wystarczajƒÖcƒÖ ilo≈õƒá kredyt√≥w.")
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Weryfikacja klucza OpenAI/kredyt√≥w.")
    except Exception as e:
        st.error(f"Nieoczekiwany b≈ÇƒÖd podczas generowania embedding√≥w OpenAI: {e}")
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Nieznany b≈ÇƒÖd.")


# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """
    Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (szybki).
    """
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", # Szybki model od Groq
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM."

# ------------------------------
# Load data from CSV
# ------------------------------
@st.cache_data
def load_and_prepare_data():
    """Loads data from knowledge.csv and combines it into a single text."""
    try:
        # Assumes knowledge.csv exists and has '≈πr√≥d≈Ço' and 'Opis' columns
        df = pd.read_csv("knowledge.csv")
        
        # Zak≈Çadamy, ≈ºe kolumny to 'Opis' i '≈πr√≥d≈Ço' i zmieniamy nazwƒô na Kategoria
        df.columns = ['Opis', 'Kategoria'] 
        
        # ≈ÅƒÖczymy kolumny 'Kategoria' i 'Opis' w jeden ciƒÖg dla ka≈ºdego wiersza
        document_texts = [
            f"Kategoria: {row['Kategoria']}. Opis: {row['Opis']}" 
            for index, row in df.iterrows()
        ]
        return document_texts
    except FileNotFoundError:
        # W przypadku braku pliku u≈ºywamy awaryjnej, wbudowanej bazy wiedzy
        st.warning("Nie znaleziono pliku 'knowledge.csv'. U≈ºywam wbudowanej, awaryjnej bazy wiedzy.")
        return [
            "Python jest jƒôzykiem programowania u≈ºywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
            "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie bez znajomo≈õci HTML/CSS/JS.",
            "Groq oferuje bardzo szybkie i darmowe modele AI dla programist√≥w, dzia≈ÇajƒÖce na specjalistycznych akceleratorach LPU (Language Processing Unit).",
            "Podobie≈Ñstwo Kosinusowe mierzy kƒÖt miƒôdzy dwoma wektorami w przestrzeni, okre≈õlajƒÖc podobie≈Ñstwo semantyczne.",
            "Do wspinaczki sportowej niezbƒôdna jest dynamika si≈Çy, kt√≥rƒÖ mo≈ºna ƒáwiczyƒá poprzez Campus Board, skoki na chwytach oraz trening pliometryczny.",
        ]
    except Exception as e:
        st.error(f"B≈ÇƒÖd ≈Çadowania knowledge.csv: {e}. U≈ºywam awaryjnej bazy wiedzy.")
        return [
            "WystƒÖpi≈Ç b≈ÇƒÖd podczas parsowania danych. Skupmy siƒô na Groq i Embeddingach.",
            "System RAG sk≈Çada siƒô z dw√≥ch g≈Ç√≥wnych etap√≥w: Retrieval (wyszukiwanie kontekstu) i Generation (generowanie odpowiedzi).",
        ]

# ------------------------------
# Simple semantic search (Cosine Similarity)
# ------------------------------
def search(query):
    """Wyszukuje najbardziej podobny dokument do zapytania."""
    # 1. Generowanie embeddingu dla zapytania - U≈ºywa compute_embeddings (OpenAI)
    try:
        # Pamiƒôtaj, ≈ºe compute_embeddings rzuca Runtime Error, je≈õli klucz OpenAI jest z≈Çy
        query_emb_list = compute_embeddings([query])
    except RuntimeError:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0 
        
    if not query_emb_list:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # 2. Obliczanie podobie≈Ñstwa
    # Funkcja z scikit-learn jest u≈ºywana do szybkiego obliczania
    # Wymagane jest przekszta≈Çcenie do numpy array i dopasowanie kszta≈Çt√≥w
    doc_embeddings_array = np.array(DOCUMENT_EMB).astype(np.float64)
    query_emb_array = np.array(query_emb).reshape(1, -1)
    
    similarities = cosine_similarity(query_emb_array, doc_embeddings_array)
    best = np.argmax(similarities)
    
    return DOCUMENT_TEXTS[best], similarities[0, best]

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("üß† Silnik Wiedzy (RAG) ‚Äî Stabilna Edycja üöÄ")

st.markdown("LLM (Llama 3 70B) dzia≈Ça na Groq. Embeddingi (wektoryzacja) dzia≈ÇajƒÖ na **stabilnym API OpenAI**.")
st.markdown("---")


# 1. PRZYGOTOWANIE BAZY WIEDZY
DOCUMENT_TEXTS = load_and_prepare_data()

# ≈Åadowanie i buforowanie embedding√≥w (z zabezpieczeniem)
@st.cache_resource
def load_document_embeddings(doc_texts):
    """Wczytuje embeddingi i zapewnia, ≈ºe aplikacja siƒô nie uruchomi, je≈õli to siƒô nie powiedzie."""
    if not doc_texts:
        return []

    st.subheader("Faza 1: Wczytywanie i wektoryzacja bazy wiedzy")
    with st.spinner(f"Generowanie embedding√≥w dla {len(doc_texts)} dokument√≥w..."):
        try:
            emb = compute_embeddings(doc_texts)
        except RuntimeError:
            st.warning("Aplikacja zosta≈Ça zatrzymana z powodu b≈Çƒôdu klucza API OpenAI. Sprawd≈∫ logi.")
            st.stop()
            
        st.success(f"Baza wiedzy (zawierajƒÖca {len(emb)} wektor√≥w) za≈Çadowana pomy≈õlnie!")
        return emb

# Wywo≈Çanie funkcji ≈Çadowania - je≈õli zawiedzie, aplikacja siƒô zatrzyma
DOCUMENT_EMB = load_document_embeddings(DOCUMENT_TEXTS)

# ------------------------------
# Phase 2: UI Input and response generation
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest Streamlit i do czego s≈Çu≈ºy?):")

if query:
    if not DOCUMENT_EMB:
        st.warning("Nie mo≈ºna wykonaƒá wyszukiwania, poniewa≈º baza wiedzy jest pusta.")
    else:
        with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
            
            # Wyszukiwanie semantyczne
            best_doc, score = search(query)

            st.markdown("### üîé Znaleziony Kontekst (RAG Retrieval)")
            st.write(f"**Podobie≈Ñstwo (Cosine Score):** {score:.4f}")
            st.code(best_doc, language='text') 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym i wspinaczkowym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
            # Wywo≈Çanie LLM (Groq)
            answer = ask_llm(final_prompt)
            st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B - Groq)")
            st.info(answer)
