import streamlit as st
# Wymagane tylko podstawowe biblioteki i klient Groq
from groq import Groq
import numpy as np 

# --- KONFIGURACJA STRONY I KLUCZY ---

st.set_page_config(layout="centered", page_title="Silnik Wiedzy RAG (Minimalistyczny Groq)")

st.title("üß† Silnik Wiedzy RAG ‚Äî Minimalistyczna Edycja Groq üöÄ")

try:
    # Klucz GROQ API musi byƒá ustawiony w Streamlit Secrets
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets. Jest wymagany dla LLM (Llama 3).")
    st.stop() 

try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# WBUDOWANA BAZA WIEDZY (KONIEC Z knowledge.csv!)
# ------------------------------
# Ta lista tekst√≥w s≈Çu≈ºy jako prosta baza wiedzy RAG.
DOCUMENT_TEXTS = [
    "Python jest jƒôzykiem programowania u≈ºywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
    "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie.",
    "Groq oferuje bardzo szybkie modele AI dla programist√≥w, dzia≈ÇajƒÖce na akceleratorach LPU (Language Processing Unit).",
    "RAG (Retrieval-Augmented Generation) to architektura AI, kt√≥ra wykorzystuje bazƒô wiedzy (retrieval) do ulepszania odpowiedzi LLM (generation).",
    "Do wspinaczki sportowej niezbƒôdna jest lina dynamiczna, uprzƒÖ≈º, ekspresy i √≥semka. Asekuracja odbywa siƒô z g√≥ry lub z do≈Çu.",
    "Wspinaczka tradycyjna wymaga umiejƒôtno≈õci osadzania w≈Çasnej asekuracji, np. ko≈õci i friend√≥w. Jest to bardziej wymagajƒÖce psychicznie i sprzƒôtowo.",
    "Najczƒôstsze b≈Çƒôdy w Streamlit to brak klucza API, nieu≈ºywanie st.cache_data/st.cache_resource oraz problemy z zale≈ºno≈õciami w requirements.txt.",
]

# ------------------------------
# PROSTE WYSZUKIWANIE PO S≈ÅOWACH KLUCZOWYCH (BEZ SKLEARN/EMBEDDINGS)
# ------------------------------
def simple_keyword_search(query, doc_texts):
    """
    Wyszukuje najbardziej pasujƒÖcy dokument na podstawie liczby wsp√≥lnych s≈Ç√≥w kluczowych.
    Jest to ZASTƒòPCZY RAG, kt√≥ry nie wymaga zewnƒôtrznych bibliotek.
    """
    # Tokenizacja i normalizacja zapytania
    query_words = set(query.lower().split())
    
    best_doc = ""
    max_matches = 0

    for doc in doc_texts:
        # Tokenizacja i normalizacja dokumentu
        doc_words = set(doc.lower().split())
        
        # Obliczanie liczby wsp√≥lnych s≈Ç√≥w (prosta metryka dopasowania)
        matches = len(query_words.intersection(doc_words))
        
        if matches > max_matches:
            max_matches = matches
            best_doc = doc
            
    return best_doc, max_matches


# ------------------------------
# GENEROWANIE ODPOWIEDZI LLM (GROQ)
# ------------------------------
def ask_llm(prompt):
    """Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (Groq)."""
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", 
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM (Groq)."

# ------------------------------
# INTERFEJS U≈ªYTKOWNIKA STREAMLIT
# ------------------------------

st.write("Ta wersja jest **ca≈Çkowicie samowystarczalna**. Nie wymaga **knowledge.csv** ani zewnƒôtrznych bibliotek (typu `sklearn`).")
st.write("U≈ºywa prostego wyszukiwania s≈Ç√≥w kluczowych (Groq-Only RAG).")
st.markdown("---")


# Faza 1: Baza Wiedzy
st.subheader("Faza 1: Baza wiedzy")
st.success(f"Baza wiedzy za≈Çadowana pomy≈õlnie! ({len(DOCUMENT_TEXTS)} dokument√≥w wbudowanych w kod.)")


# Faza 2: Zapytanie
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Co to jest RAG?):")

if query:
    with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
        
        # 1. Wyszukiwanie kluczowe
        best_doc, matches = simple_keyword_search(query, DOCUMENT_TEXTS)

        st.markdown("### üîé Znaleziony Kontekst")
        st.write(f"**Liczba pasujƒÖcych s≈Ç√≥w kluczowych:** {matches}")
        
        if matches == 0:
            st.warning("Brak pasujƒÖcych s≈Ç√≥w kluczowych. Model odpowie bez kontekstu.")
            # Prompt dla braku kontekstu (pytanie otwarte)
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym. Postaraj siƒô odpowiedzieƒá na pytanie.
            Je≈õli nie masz pewno≈õci, odpowiedz: 'Nie jestem w stanie precyzyjnie odpowiedzieƒá na to pytanie bez kontekstu w mojej bazie wiedzy.'
            Pytanie: {query}
            Odpowied≈∫:
            """
        else:
            st.code(best_doc) 
    
            # 2. Tworzenie promptu RAG z kontekstem
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
        # 3. Wywo≈Çanie LLM (Groq)
        answer = ask_llm(final_prompt)
        st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B - Groq)")
        st.info(answer)
