import streamlit as st
import numpy as np
import json
import pandas as pd
# Importy dla Groq (LLM)
from groq import Groq
# Importy dla OpenAI (Embeddingi)
from openai import OpenAI
from openai import APIError
# Import dla podobie≈Ñstwa kosinusowego
from sklearn.metrics.pairwise import cosine_similarity 

# --- KONFIGURACJA KLUCZY I INICJALIZACJA ---

st.set_page_config(layout="centered", page_title="Silnik Wiedzy RAG")

# 1. Sprawdzamy klucz Groq (dla LLM)
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    st.error("B≈ÇƒÖd konfiguracji: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets. Jest wymagany dla LLM (Llama 3).")
    st.stop() 

# 2. Sprawdzamy i inicjalizujemy klienta OpenAI (dla Embedding√≥w)
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    if not OPENAI_API_KEY:
        st.error("B≈ÇƒÖd: Wymagana warto≈õƒá 'OPENAI_API_KEY' w Streamlit Secrets. U≈ºywamy go do wektoryzacji (Embedding√≥w).")
        st.stop()
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'OPENAI_API_KEY' w Streamlit Secrets. Jest WYMAGANY dla embedding√≥w.")
    st.stop()

# 3. Inicjalizacja klienta Groq (dla LLM)
try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - WY≈ÅƒÑCZNIE OpenAI
# ------------------------------
@st.cache_data(show_spinner=False)
def compute_embeddings(texts):
    """
    Generuje embeddingi dla listy tekst√≥w u≈ºywajƒÖc modelu OpenAI
    (text-embedding-3-small).
    """
    try:
        # st.info("Using OpenAI (text-embedding-3-small) for embeddings...")
        response = openai_client.embeddings.create(
            model="text-embedding-3-small", 
            input=texts
        )
        # Pobieranie wektor√≥w i konwersja do numpy array
        embeddings = np.array([data.embedding for data in response.data])
        return embeddings

    except APIError as e:
        # Obs≈Çuga b≈Çƒôd√≥w autoryzacji/Quota
        st.error(f"Krytyczny b≈ÇƒÖd API OpenAI (Embeddingi): {e}. Sprawd≈∫, czy klucz OPENAI_API_KEY jest poprawny i czy masz wystarczajƒÖcƒÖ ilo≈õƒá kredyt√≥w.")
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Weryfikacja klucza OpenAI/kredyt√≥w.")
    except Exception as e:
        st.error(f"Nieoczekiwany b≈ÇƒÖd podczas generowania embedding√≥w OpenAI: {e}")
        raise RuntimeError("B≈ÇƒÖd wektoryzacji: Nieznany b≈ÇƒÖd.")


# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """
    Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (Groq).
    """
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", # Szybki model od Groq
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM."

# ------------------------------
# Load data from CSV
# ------------------------------
@st.cache_data
def load_and_prepare_data():
    """Wczytuje dane z knowledge.csv i ≈ÇƒÖczy je w jeden ciƒÖg tekstowy dla ka≈ºdego dokumentu."""
    try:
        # Wczytywanie pliku CSV
        df = pd.read_csv("knowledge.csv")
        
        # Zak≈Çadamy, ≈ºe kolumny to 'Opis' i '≈πr√≥d≈Ço'
        df.columns = ['Opis', 'Kategoria'] # Tymczasowa zmiana nazwy dla sp√≥jno≈õci
        
        # ≈ÅƒÖczymy kolumny, tworzƒÖc ustrukturyzowany dokument tekstowy
        document_texts = [
            f"Kategoria: {row['Kategoria']}. Opis: {row['Opis']}" 
            for index, row in df.iterrows()
        ]
        return document_texts
    except FileNotFoundError:
        st.error("B≈ÇƒÖd: Nie znaleziono pliku 'knowledge.csv'. Upewnij siƒô, ≈ºe znajduje siƒô w tym samym katalogu co aplikacja.")
        # W przypadku b≈Çƒôdu zatrzymujemy aplikacjƒô, poniewa≈º ten plik nie ma fallbacku
        st.stop() 
    except Exception as e:
        st.error(f"B≈ÇƒÖd ≈Çadowania knowledge.csv: {e}")
        st.stop()
        return []

# ------------------------------
# Simple semantic search (Cosine Similarity)
# ------------------------------
def search(query):
    """Wyszukuje najbardziej podobny dokument do zapytania za pomocƒÖ Podobie≈Ñstwa Kosinusowego."""
    # 1. Generowanie embeddingu dla zapytania
    try:
        query_emb_list = compute_embeddings([query])
    except RuntimeError:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0 
        
    if query_emb_list.size == 0:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # 2. Obliczanie podobie≈Ñstwa
    # U≈ºywamy zaimplementowanej funkcji cosine_similarity z scikit-learn
    # Reshape jest konieczny, bo cosine_similarity oczekuje 2D tablic
    document_embeddings_np = DOCUMENT_EMB.astype(np.float64) # Upewnienie siƒô co do typu
    similarities = cosine_similarity(query_emb.reshape(1, -1), document_embeddings_np)
    best = np.argmax(similarities)
    
    return DOCUMENT_TEXTS[best], similarities[0, best]

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("üß† Silnik Wiedzy (RAG) ‚Äî Stabilna Edycja üöÄ")

st.markdown("LLM (Llama 3 70B) dzia≈Ça na Groq. Embeddingi dzia≈ÇajƒÖ na **stabilnym API OpenAI**.")
st.markdown("Wersja wymaga pliku **`knowledge.csv`** do za≈Çadowania bazy wiedzy.")
st.markdown("---")


# 1. PRZYGOTOWANIE BAZY WIEDZY
DOCUMENT_TEXTS = load_and_prepare_data()

# ≈Åadowanie i buforowanie embedding√≥w (z zabezpieczeniem)
@st.cache_resource
def load_document_embeddings(doc_texts):
    """Wczytuje embeddingi i zapewnia, ≈ºe aplikacja siƒô nie uruchomi, je≈õli to siƒô nie powiedzie."""
    if not doc_texts:
        return np.array([]) # Zwracamy pustƒÖ tablicƒô numpy

    st.subheader("Faza 1: Wczytywanie i wektoryzacja bazy wiedzy")
    with st.spinner(f"Generowanie embedding√≥w dla {len(doc_texts)} dokument√≥w..."):
        try:
            emb = compute_embeddings(doc_texts)
        except RuntimeError:
            st.warning("Aplikacja zosta≈Ça zatrzymana z powodu b≈Çƒôdu klucza API OpenAI. Sprawd≈∫ logi.")
            st.stop()
            
        st.success(f"Baza wiedzy (zawierajƒÖca {len(emb)} wektor√≥w) za≈Çadowana pomy≈õlnie!")
        return emb

# Wywo≈Çanie funkcji ≈Çadowania - je≈õli zawiedzie, aplikacja siƒô zatrzyma
DOCUMENT_EMB = load_document_embeddings(DOCUMENT_TEXTS)

# ------------------------------
# Phase 2: UI Input and response generation
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest RAG i dlaczego wymaga dw√≥ch kluczy API?):")

if query:
    if DOCUMENT_EMB.size == 0:
        st.warning("Nie mo≈ºna wykonaƒá wyszukiwania, poniewa≈º baza wiedzy jest pusta lub wystƒÖpi≈Ç b≈ÇƒÖd ≈Çadowania.")
    else:
        with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
            
            # Wyszukiwanie semantyczne
            best_doc, score = search(query)

            st.markdown("### üîé Znaleziony Kontekst (RAG Retrieval)")
            st.write(f"**Podobie≈Ñstwo (Cosine Score):** {score:.4f}")
            st.code(best_doc, language='text') 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
            # Wywo≈Çanie LLM (Groq)
            answer = ask_llm(final_prompt)
            st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B - Groq)")
            st.info(answer)


Teraz masz drugi plik, **`rag_engine_app.py`**, r√≥wnie≈º osadzony bezpo≈õrednio w czacie. Pamiƒôtaj, ≈ºe ten plik wymaga r√≥wnie≈º pliku **`knowledge.csv`** w tym samym katalogu, aby m√≥c dzia≈Çaƒá poprawnie.

Je≈õli potrzebujesz pliku `knowledge.csv` (zak≈ÇadajƒÖc, ≈ºe go nie masz), oto jego tre≈õƒá:


http://googleusercontent.com/immersive_entry_chip/0
