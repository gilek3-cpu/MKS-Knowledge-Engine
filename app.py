import streamlit as st
import numpy as np
import json
# Importujemy Groq dla LLM
from groq import Groq
# Importujemy OpenAI dla opcjonalnego embeddingu
from openai import OpenAI

# --- KONFIGURACJA KLUCZY ---
# 1. Klucz Groq (dla Llama 3)
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except KeyError:
    st.error("B≈ÇƒÖd: Brak klucza 'GROQ_API_KEY' w Streamlit Secrets.")
    st.stop() 

# 2. Klucz OpenAI (dla embedding√≥w - do testowania problemu Groq)
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except KeyError:
    st.warning("Uwaga: Brak klucza 'OPENAI_API_KEY'. Pr√≥ba u≈ºycia Groq dla embedding√≥w.")
    openai_client = None

# Inicjalizacja klienta Groq
try:
    client = Groq(api_key=GROQ_API_KEY)
except Exception as e:
    st.error(f"B≈ÇƒÖd inicjalizacji klienta Groq: {e}")
    st.stop()


# ------------------------------
# EMBEDDINGS (Wektoryzacja) - U≈ºywamy OpenAI jako fallback
# ------------------------------
@st.cache_data
def compute_embeddings(texts):
    """
    Generuje embeddingi dla listy tekst√≥w. Domy≈õlnie u≈ºywa Groq, ale u≈ºywa OpenAI,
    je≈õli Groq zwraca b≈ÇƒÖd (np. 404 model_not_found) lub je≈õli klucz OpenAI jest dostƒôpny
    i Groq jest skonfigurowany, by u≈ºyƒá fallbacku.
    """
    embeddings = []
    
    # --- PR√ìBA U≈ªYCIA GROQ ---
    if not openai_client:
        try:
            # W tym try-except Groq jest u≈ºywany jako g≈Ç√≥wny mechanizm.
            st.info("U≈ºywam Groq (nomic-embed-text) dla embedding√≥w...")
            for t in texts:
                response = client.embeddings.create(
                    model="nomic-embed-text",
                    input=t
                )
                embeddings.append(response.data[0].embedding)
            st.success("Groq Embeddings sukces!")
            return embeddings
        
        except Exception as e:
            # Je≈õli Groq zawiedzie (jak w Twoim przypadku) i nie mamy klucza OpenAI, to ko≈Ñczymy.
            st.error(f"Krytyczny b≈ÇƒÖd API Groq w compute_embeddings. B≈ÇƒÖd: {e}")
            raise RuntimeError("Nie uda≈Ço siƒô wygenerowaƒá embedding√≥w. (Brak fallbacku OpenAI).")


    # --- U≈ªYCIE OPENAI JAKO FALLBACK (JE≈öLI KLUCZ OPENAI JEST DOSTƒòPNY) ---
    else:
        st.info("U≈ºywam OpenAI (text-embedding-3-small) dla embedding√≥w (jako fallback)...")
        try:
            # Wywo≈Çujemy API OpenAI
            response = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=texts
            )
            embeddings = [data.embedding for data in response.data]
            st.success("OpenAI Embeddings sukces!")
            return embeddings

        except Exception as e:
            # Je≈õli OpenAI te≈º zawiedzie (co jest ma≈Ço prawdopodobne), rzucamy b≈ÇƒÖd.
            st.error(f"Krytyczny b≈ÇƒÖd API OpenAI w compute_embeddings. B≈ÇƒÖd: {e}")
            raise RuntimeError("Nie uda≈Ço siƒô wygenerowaƒá embedding√≥w (fallback nieudany).")


# ------------------------------
# LLM Response (using Groq)
# ------------------------------
def ask_llm(prompt):
    """
    Generuje odpowied≈∫ LLM na podstawie promptu, u≈ºywajƒÖc modelu Llama 3 70B (szybki).
    """
    try:
        completion = client.chat.completions.create(
            model="llama3-70b-8192", # Szybki model od Groq
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return completion.choices[0].message.content
    except Exception as e:
        st.error(f"B≈ÇƒÖd podczas wywo≈Çania LLM Groq: {e}")
        return "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd w komunikacji z modelem LLM."

# ------------------------------
# Streamlit UI
# ------------------------------
st.title("üß† Silnik Wiedzy ‚Äî Groq Edition üöÄ")

# Zmodyfikowany tekst UI, aby odzwierciedliƒá u≈ºycie OpenAI/Groq
if openai_client:
    st.write("Embeddingi (OpenAI) + LLM (Llama 3 70B) - **Test Fallbacku Embeddings**.")
else:
    st.write("Embeddingi (Nomic Embed) + LLM (Llama 3 70B) - **Wszystko na Groq API**.")

st.markdown("---")


# Example documents
DOCUMENT_TEXTS = [
    "Python jest jƒôzykiem programowania u≈ºywanym do analizy danych, uczenia maszynowego i tworzenia aplikacji webowych.",
    "Streamlit to darmowy framework do budowy interaktywnych aplikacji webowych w Pythonie bez znajomo≈õci HTML/CSS/JS.",
    "Groq oferuje bardzo szybkie i darmowe modele AI dla programist√≥w, dzia≈ÇajƒÖce na specjalistycznych akceleratorach LPU (Language Processing Unit).",
    "Funkcja Cosine Similarity mierzy kƒÖt miƒôdzy dwoma wektorami w przestrzeni, okre≈õlajƒÖc podobie≈Ñstwo semantyczne.",
]

# ≈Åadowanie i buforowanie embedding√≥w (z zabezpieczeniem)
@st.cache_resource
def load_document_embeddings():
    """Wczytuje embeddingi i zapewnia, ≈ºe aplikacja siƒô nie uruchomi, je≈õli to siƒô nie powiedzie."""
    st.subheader("Faza 1: Wczytywanie bazy wiedzy")
    with st.spinner("Generowanie embedding√≥w dla dokument√≥w..."):
        try:
            emb = compute_embeddings(DOCUMENT_TEXTS)
        except RuntimeError:
            # Wy≈õwietla b≈ÇƒÖd rzucony przez compute_embeddings
            st.error("Nie uda≈Ço siƒô za≈Çadowaƒá bazy wiedzy. Sprawd≈∫ klucze API i logi b≈Çƒôd√≥w.")
            st.stop()
            
        st.success("Baza wiedzy za≈Çadowana pomy≈õlnie!")
        return emb

DOCUMENT_EMB = load_document_embeddings()

# ------------------------------
# Simple semantic search (Cosine Similarity)
# ------------------------------
def cosine_similarity(a, b):
    """Oblicza podobie≈Ñstwo cosinusowe miƒôdzy dwoma wektorami."""
    a = np.array(a)
    b = np.array(b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
        
    return np.dot(a, b) / (norm_a * norm_b)

def search(query):
    """Wyszukuje najbardziej podobny dokument do zapytania."""
    # 1. Generowanie embeddingu dla zapytania
    try:
        query_emb_list = compute_embeddings([query])
    except RuntimeError:
        return "B≈ÇƒÖd generowania wektora zapytania.", 0.0

    query_emb = query_emb_list[0]
    
    # 2. Obliczanie podobie≈Ñstwa
    sims = [cosine_similarity(query_emb, emb) for emb in DOCUMENT_EMB]
    best = np.argmax(sims)
    
    return DOCUMENT_TEXTS[best], sims[best]

# ------------------------------
# UI Input
# ------------------------------
st.subheader("Faza 2: Zapytanie do Silnika Wiedzy")
query = st.text_input("Zadaj pytanie (np. Czym jest Streamlit?):")

if query:
    if not DOCUMENT_EMB:
        st.warning("Nie mo≈ºna wykonaƒá wyszukiwania, poniewa≈º baza wiedzy jest pusta.")
    else:
        with st.spinner("Szukam kontekstu i generujƒô odpowied≈∫..."):
            
            # Wyszukiwanie semantyczne
            best_doc, score = search(query)

            st.markdown("### üîé Znaleziony Kontekst")
            st.write(f"**Podobie≈Ñstwo (Cosine Score):** {score:.4f}")
            st.code(best_doc) 
    
            # Tworzenie promptu RAG
            final_prompt = f"""
            Jeste≈õ ekspertem technicznym. U≈ºyj **wy≈ÇƒÖcznie** poni≈ºszego fragmentu wiedzy, 
            aby odpowiedzieƒá na pytanie u≈ºytkownika. Odpowiadaj zwiƒô≈∫le i precyzyjnie. 
            Je≈õli kontekst nie zawiera odpowiedzi, odpowiedz: 'Brak wystarczajƒÖcych informacji w bazie wiedzy.'.
    
            Pytanie:
            {query}
    
            Kontekst:
            {best_doc}
    
            Odpowied≈∫:
            """
    
            # Wywo≈Çanie LLM
            answer = ask_llm(final_prompt)
            st.markdown("### ü§ñ Odpowied≈∫ Modelu (Llama 3 70B)")
            st.info(answer)
